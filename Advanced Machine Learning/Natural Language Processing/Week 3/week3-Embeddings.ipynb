{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    res = np.zeros(dim)\n",
    "    n = 0\n",
    "    for word in question.split(' '):\n",
    "        if word in embeddings:\n",
    "            res += embeddings[word]\n",
    "            n += 1\n",
    "    if n == 0:\n",
    "        return np.zeros(dim)\n",
    "    else:\n",
    "        return res / float(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    n = len(dup_ranks)\n",
    "    for rank in dup_ranks:\n",
    "        if rank <= k:\n",
    "            res += 1\n",
    "    return float(res) / float(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    n = len(dup_ranks)\n",
    "    res = 0.\n",
    "    for rank in dup_ranks:\n",
    "        if rank <= k:\n",
    "            res += 1 / np.log2(1 + rank)\n",
    "    return float(res) / float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    mean_question = question_to_vec(question, embeddings, dim)\n",
    "    mean_embeddings = [question_to_vec(candidate, embeddings, dim) for candidate in candidates]\n",
    "    similarities = cosine_similarity([mean_question], mean_embeddings)\n",
    "#     print(similarities)\n",
    "    result = sorted(list(enumerate(candidates)), key=lambda x: -similarities[0,x[0]])\n",
    "#     print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.217 | Hits@   1: 0.217\n",
      "DCG@   5: 0.271 | Hits@   5: 0.319\n",
      "DCG@  10: 0.288 | Hits@  10: 0.370\n",
      "DCG@ 100: 0.326 | Hits@ 100: 0.560\n",
      "DCG@ 500: 0.358 | Hits@ 500: 0.818\n",
      "DCG@1000: 0.378 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    ######### YOUR CODE HERE #############\n",
    "    prepared_validation.append([text_prepare(i) for i in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.320 | Hits@   1: 0.320\n",
      "DCG@   5: 0.387 | Hits@   5: 0.448\n",
      "DCG@  10: 0.404 | Hits@  10: 0.500\n",
      "DCG@ 100: 0.438 | Hits@ 100: 0.669\n",
      "DCG@ 500: 0.460 | Hits@ 500: 0.844\n",
      "DCG@1000: 0.477 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file(\"data/train.tsv\", \"data/train_prepared.tsv\")\n",
    "prepare_file(\"data/test.tsv\", \"data/test_prepared.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = \"data/test_prepared.tsv\" ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TRAINING HAPPENING HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: .040000  loss: 0.008723  eta: 0h10m  tot: 0h2m40s  (20.0%)%  lr: 0.049640  loss: 0.029136  eta: 0h17m  tot: 0h0m8s  (0.7%)3.7%  lr: 0.049630  loss: 0.028798  eta: 0h17m  tot: 0h0m8s  (0.7%)4.3%  lr: 0.049540  loss: 0.027388  eta: 0h17m  tot: 0h0m9s  (0.9%)5.3%  lr: 0.049449  loss: 0.025325  eta: 0h16m  tot: 0h0m11s  (1.1%)5.9%  lr: 0.049419  loss: 0.024210  eta: 0h16m  tot: 0h0m12s  (1.2%)6.0%  lr: 0.049419  loss: 0.024127  eta: 0h16m  tot: 0h0m12s  (1.2%)8.0%  lr: 0.049169  loss: 0.022115  eta: 0h15m  tot: 0h0m15s  (1.6%)9.1%  lr: 0.049029  loss: 0.021244  eta: 0h15m  tot: 0h0m17s  (1.8%)10.6%  lr: 0.048869  loss: 0.019858  eta: 0h14m  tot: 0h0m20s  (2.1%)11.4%  lr: 0.048839  loss: 0.019501  eta: 0h14m  tot: 0h0m21s  (2.3%)12.0%  lr: 0.048769  loss: 0.019267  eta: 0h14m  tot: 0h0m22s  (2.4%)12.3%  lr: 0.048739  loss: 0.019020  eta: 0h14m  tot: 0h0m22s  (2.5%)12.6%  lr: 0.048739  loss: 0.018922  eta: 0h14m  tot: 0h0m23s  (2.5%)12.9%  lr: 0.048709  loss: 0.018795  eta: 0h14m  tot: 0h0m23s  (2.6%)13.0%  lr: 0.048699  loss: 0.018775  eta: 0h14m  tot: 0h0m23s  (2.6%)13.7%  lr: 0.048639  loss: 0.018418  eta: 0h14m  tot: 0h0m24s  (2.7%)14.6%  lr: 0.048539  loss: 0.017938  eta: 0h14m  tot: 0h0m26s  (2.9%)15.8%  lr: 0.048338  loss: 0.017292  eta: 0h14m  tot: 0h0m28s  (3.2%)16.4%  lr: 0.048278  loss: 0.017119  eta: 0h14m  tot: 0h0m29s  (3.3%)17.6%  lr: 0.048148  loss: 0.016646  eta: 0h14m  tot: 0h0m31s  (3.5%)17.7%  lr: 0.048118  loss: 0.016582  eta: 0h14m  tot: 0h0m31s  (3.5%)%  lr: 0.048098  loss: 0.016476  eta: 0h14m  tot: 0h0m32s  (3.6%)18.1%  lr: 0.048078  loss: 0.016447  eta: 0h14m  tot: 0h0m32s  (3.6%)21.0%  lr: 0.047878  loss: 0.015472  eta: 0h13m  tot: 0h0m37s  (4.2%)23.2%  lr: 0.047648  loss: 0.014894  eta: 0h13m  tot: 0h0m40s  (4.6%)24.9%  lr: 0.047478  loss: 0.014478  eta: 0h13m  tot: 0h0m43s  (5.0%)27.0%  lr: 0.047307  loss: 0.014066  eta: 0h13m  tot: 0h0m46s  (5.4%)27.1%  lr: 0.047297  loss: 0.014044  eta: 0h13m  tot: 0h0m47s  (5.4%)28.3%  lr: 0.047177  loss: 0.013822  eta: 0h13m  tot: 0h0m49s  (5.7%)28.5%  lr: 0.047157  loss: 0.013761  eta: 0h13m  tot: 0h0m49s  (5.7%)28.7%  lr: 0.047137  loss: 0.013735  eta: 0h13m  tot: 0h0m49s  (5.7%)30.3%  lr: 0.047047  loss: 0.013509  eta: 0h13m  tot: 0h0m52s  (6.1%)31.1%  lr: 0.047007  loss: 0.013387  eta: 0h13m  tot: 0h0m53s  (6.2%)32.5%  lr: 0.046907  loss: 0.013135  eta: 0h13m  tot: 0h0m55s  (6.5%)33.4%  lr: 0.046847  loss: 0.012969  eta: 0h13m  tot: 0h0m57s  (6.7%)33.6%  lr: 0.046817  loss: 0.012939  eta: 0h13m  tot: 0h0m57s  (6.7%)34.7%  lr: 0.046677  loss: 0.012775  eta: 0h13m  tot: 0h0m59s  (6.9%)34.9%  lr: 0.046657  loss: 0.012754  eta: 0h13m  tot: 0h0m59s  (7.0%)35.0%  lr: 0.046637  loss: 0.012754  eta: 0h13m  tot: 0h0m59s  (7.0%)35.1%  lr: 0.046617  loss: 0.012738  eta: 0h13m  tot: 0h0m59s  (7.0%)35.4%  lr: 0.046597  loss: 0.012697  eta: 0h13m  tot: 0h1m0s  (7.1%)35.6%  lr: 0.046567  loss: 0.012670  eta: 0h12m  tot: 0h1m0s  (7.1%)36.3%  lr: 0.046487  loss: 0.012595  eta: 0h12m  tot: 0h1m1s  (7.3%)37.4%  lr: 0.046366  loss: 0.012454  eta: 0h12m  tot: 0h1m3s  (7.5%)38.7%  lr: 0.046216  loss: 0.012286  eta: 0h12m  tot: 0h1m5s  (7.7%)41.4%  lr: 0.046026  loss: 0.011978  eta: 0h12m  tot: 0h1m9s  (8.3%)0.011951  eta: 0h12m  tot: 0h1m9s  (8.3%)42.0%  lr: 0.045956  loss: 0.011916  eta: 0h12m  tot: 0h1m10s  (8.4%)42.6%  lr: 0.045866  loss: 0.011838  eta: 0h12m  tot: 0h1m11s  (8.5%)44.0%  lr: 0.045676  loss: 0.011711  eta: 0h12m  tot: 0h1m13s  (8.8%)44.7%  lr: 0.045606  loss: 0.011627  eta: 0h12m  tot: 0h1m15s  (8.9%)%  lr: 0.045586  loss: 0.011556  eta: 0h12m  tot: 0h1m16s  (9.1%)47.1%  lr: 0.045335  loss: 0.011406  eta: 0h12m  tot: 0h1m18s  (9.4%)%  lr: 0.045215  loss: 0.011327  eta: 0h12m  tot: 0h1m20s  (9.6%)48.4%  lr: 0.045165  loss: 0.011304  eta: 0h12m  tot: 0h1m21s  (9.7%)49.3%  lr: 0.045085  loss: 0.011226  eta: 0h12m  tot: 0h1m22s  (9.9%)50.1%  lr: 0.045015  loss: 0.011179  eta: 0h12m  tot: 0h1m23s  (10.0%)52.2%  lr: 0.044855  loss: 0.010990  eta: 0h12m  tot: 0h1m27s  (10.4%)52.3%  lr: 0.044835  loss: 0.010996  eta: 0h12m  tot: 0h1m27s  (10.5%)52.8%  lr: 0.044765  loss: 0.010969  eta: 0h12m  tot: 0h1m28s  (10.6%)54.3%  lr: 0.044635  loss: 0.010870  eta: 0h12m  tot: 0h1m30s  (10.9%)54.4%  lr: 0.044625  loss: 0.010859  eta: 0h12m  tot: 0h1m30s  (10.9%)54.6%  lr: 0.044625  loss: 0.010841  eta: 0h12m  tot: 0h1m31s  (10.9%)55.7%  lr: 0.044495  loss: 0.010781  eta: 0h12m  tot: 0h1m33s  (11.1%)55.9%  lr: 0.044475  loss: 0.010774  eta: 0h12m  tot: 0h1m33s  (11.2%)56.4%  lr: 0.044425  loss: 0.010750  eta: 0h12m  tot: 0h1m34s  (11.3%)56.7%  lr: 0.044324  loss: 0.010722  eta: 0h12m  tot: 0h1m34s  (11.3%)57.8%  lr: 0.044244  loss: 0.010662  eta: 0h12m  tot: 0h1m36s  (11.6%)58.0%  lr: 0.044234  loss: 0.010651  eta: 0h12m  tot: 0h1m36s  (11.6%)58.6%  lr: 0.044194  loss: 0.010607  eta: 0h12m  tot: 0h1m37s  (11.7%)58.7%  lr: 0.044144  loss: 0.010608  eta: 0h12m  tot: 0h1m38s  (11.7%)58.8%  lr: 0.044134  loss: 0.010601  eta: 0h12m  tot: 0h1m38s  (11.8%)60.9%  lr: 0.043874  loss: 0.010474  eta: 0h12m  tot: 0h1m41s  (12.2%)61.8%  lr: 0.043754  loss: 0.010398  eta: 0h12m  tot: 0h1m42s  (12.4%)62.1%  lr: 0.043734  loss: 0.010380  eta: 0h12m  tot: 0h1m43s  (12.4%)63.2%  lr: 0.043624  loss: 0.010311  eta: 0h11m  tot: 0h1m44s  (12.6%)0.043604  loss: 0.010304  eta: 0h11m  tot: 0h1m44s  (12.7%)63.5%  lr: 0.043594  loss: 0.010296  eta: 0h11m  tot: 0h1m45s  (12.7%)66.2%  lr: 0.043343  loss: 0.010128  eta: 0h11m  tot: 0h1m48s  (13.2%)66.7%  lr: 0.043263  loss: 0.010092  eta: 0h11m  tot: 0h1m49s  (13.3%)67.7%  lr: 0.043183  loss: 0.010030  eta: 0h11m  tot: 0h1m51s  (13.5%)68.1%  lr: 0.043143  loss: 0.010018  eta: 0h11m  tot: 0h1m52s  (13.6%)68.4%  lr: 0.043123  loss: 0.010005  eta: 0h11m  tot: 0h1m52s  (13.7%)69.2%  lr: 0.043023  loss: 0.009951  eta: 0h11m  tot: 0h1m53s  (13.8%)70.4%  lr: 0.042933  loss: 0.009895  eta: 0h11m  tot: 0h1m55s  (14.1%)72.2%  lr: 0.042793  loss: 0.009807  eta: 0h11m  tot: 0h1m58s  (14.4%)72.7%  lr: 0.042743  loss: 0.009767  eta: 0h11m  tot: 0h1m59s  (14.5%)75.4%  lr: 0.042513  loss: 0.009625  eta: 0h11m  tot: 0h2m4s  (15.1%)76.7%  lr: 0.042403  loss: 0.009578  eta: 0h11m  tot: 0h2m5s  (15.3%)77.1%  lr: 0.042282  loss: 0.009559  eta: 0h11m  tot: 0h2m6s  (15.4%)77.7%  lr: 0.042192  loss: 0.009534  eta: 0h11m  tot: 0h2m7s  (15.5%)15.8%)79.5%  lr: 0.041992  loss: 0.009458  eta: 0h11m  tot: 0h2m10s  (15.9%)79.6%  lr: 0.041962  loss: 0.009456  eta: 0h11m  tot: 0h2m10s  (15.9%)79.7%  lr: 0.041962  loss: 0.009450  eta: 0h11m  tot: 0h2m10s  (15.9%)80.6%  lr: 0.041862  loss: 0.009409  eta: 0h11m  tot: 0h2m12s  (16.1%)80.9%  lr: 0.041852  loss: 0.009396  eta: 0h11m  tot: 0h2m13s  (16.2%)81.0%  lr: 0.041852  loss: 0.009395  eta: 0h11m  tot: 0h2m13s  (16.2%)%  lr: 0.041832  loss: 0.009386  eta: 0h11m  tot: 0h2m13s  (16.2%)81.8%  lr: 0.041742  loss: 0.009356  eta: 0h11m  tot: 0h2m14s  (16.4%)82.2%  lr: 0.041692  loss: 0.009335  eta: 0h11m  tot: 0h2m15s  (16.4%)11m  tot: 0h2m17s  (16.7%)83.8%  lr: 0.041482  loss: 0.009283  eta: 0h11m  tot: 0h2m17s  (16.8%)h2m18s  (16.9%)85.3%  lr: 0.041261  loss: 0.009224  eta: 0h11m  tot: 0h2m19s  (17.1%)87.5%  lr: 0.041041  loss: 0.009138  eta: 0h11m  tot: 0h2m23s  (17.5%)87.9%  lr: 0.040981  loss: 0.009118  eta: 0h11m  tot: 0h2m23s  (17.6%)90.7%  lr: 0.040671  loss: 0.009037  eta: 0h11m  tot: 0h2m28s  (18.1%)91.3%  lr: 0.040601  loss: 0.009020  eta: 0h11m  tot: 0h2m29s  (18.3%)91.8%  lr: 0.040571  loss: 0.009007  eta: 0h11m  tot: 0h2m30s  (18.4%)92.7%  lr: 0.040431  loss: 0.008973  eta: 0h11m  tot: 0h2m31s  (18.5%)93.9%  lr: 0.040371  loss: 0.008929  eta: 0h11m  tot: 0h2m33s  (18.8%)94.2%  lr: 0.040351  loss: 0.008920  eta: 0h10m  tot: 0h2m33s  (18.8%)95.1%  lr: 0.040230  loss: 0.008881  eta: 0h10m  tot: 0h2m35s  (19.0%)96.1%  lr: 0.040130  loss: 0.008851  eta: 0h10m  tot: 0h2m37s  (19.2%)97.2%  lr: 0.040040  loss: 0.008817  eta: 0h10m  tot: 0h2m38s  (19.4%)0.040000  loss: 0.008722  eta: 0h10m  tot: 0h2m40s  (20.0%)\n",
      " ---+++                Epoch    0 Train error : 0.00896130 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002655  eta: 0h7m  tot: 0h5m10s  (40.0%)3%  lr: 0.039960  loss: 0.001988  eta: 0h7m  tot: 0h2m40s  (20.1%)1.4%  lr: 0.039800  loss: 0.002271  eta: 0h9m  tot: 0h2m42s  (20.3%)%  lr: 0.039700  loss: 0.002520  eta: 0h9m  tot: 0h2m43s  (20.4%)3.1%  lr: 0.039560  loss: 0.002632  eta: 0h9m  tot: 0h2m45s  (20.6%)4.9%  lr: 0.039459  loss: 0.002569  eta: 0h9m  tot: 0h2m47s  (21.0%)5.5%  lr: 0.039419  loss: 0.002668  eta: 0h9m  tot: 0h2m48s  (21.1%)6.6%  lr: 0.039299  loss: 0.002697  eta: 0h9m  tot: 0h2m50s  (21.3%)7.4%  lr: 0.039229  loss: 0.002753  eta: 0h9m  tot: 0h2m51s  (21.5%)8.4%  lr: 0.039109  loss: 0.002752  eta: 0h9m  tot: 0h2m52s  (21.7%)9.2%  lr: 0.039039  loss: 0.002658  eta: 0h9m  tot: 0h2m53s  (21.8%)9.6%  lr: 0.038989  loss: 0.002692  eta: 0h9m  tot: 0h2m54s  (21.9%)0.038819  loss: 0.002645  eta: 0h9m  tot: 0h2m57s  (22.3%)11.5%  lr: 0.038809  loss: 0.002662  eta: 0h9m  tot: 0h2m57s  (22.3%)9m  tot: 0h3m0s  (22.7%)14.1%  lr: 0.038629  loss: 0.002646  eta: 0h9m  tot: 0h3m1s  (22.8%)15.8%  lr: 0.038398  loss: 0.002661  eta: 0h9m  tot: 0h3m3s  (23.2%)16.7%  lr: 0.038348  loss: 0.002662  eta: 0h9m  tot: 0h3m4s  (23.3%)17.2%  lr: 0.038308  loss: 0.002654  eta: 0h9m  tot: 0h3m5s  (23.4%)17.9%  lr: 0.038208  loss: 0.002629  eta: 0h9m  tot: 0h3m6s  (23.6%)18.9%  lr: 0.038138  loss: 0.002644  eta: 0h9m  tot: 0h3m8s  (23.8%)19.3%  lr: 0.038118  loss: 0.002647  eta: 0h9m  tot: 0h3m8s  (23.9%)20.1%  lr: 0.038038  loss: 0.002656  eta: 0h9m  tot: 0h3m9s  (24.0%)20.3%  lr: 0.038028  loss: 0.002655  eta: 0h9m  tot: 0h3m10s  (24.1%)21.7%  lr: 0.037938  loss: 0.002654  eta: 0h9m  tot: 0h3m12s  (24.3%)25.2%  lr: 0.037518  loss: 0.002665  eta: 0h9m  tot: 0h3m17s  (25.0%)27.0%  lr: 0.037407  loss: 0.002671  eta: 0h9m  tot: 0h3m19s  (25.4%)27.2%  lr: 0.037377  loss: 0.002668  eta: 0h9m  tot: 0h3m20s  (25.4%)27.8%  lr: 0.037317  loss: 0.002657  eta: 0h9m  tot: 0h3m21s  (25.6%)28.1%  lr: 0.037267  loss: 0.002653  eta: 0h9m  tot: 0h3m21s  (25.6%)%  lr: 0.037197  loss: 0.002644  eta: 0h9m  tot: 0h3m22s  (25.7%)29.0%  lr: 0.037187  loss: 0.002651  eta: 0h9m  tot: 0h3m22s  (25.8%)29.3%  lr: 0.037177  loss: 0.002653  eta: 0h9m  tot: 0h3m23s  (25.9%)30.1%  lr: 0.037057  loss: 0.002651  eta: 0h9m  tot: 0h3m24s  (26.0%)30.2%  lr: 0.037047  loss: 0.002647  eta: 0h9m  tot: 0h3m24s  (26.0%)30.5%  lr: 0.037027  loss: 0.002636  eta: 0h9m  tot: 0h3m25s  (26.1%)33.1%  lr: 0.036667  loss: 0.002632  eta: 0h8m  tot: 0h3m28s  (26.6%)33.8%  lr: 0.036587  loss: 0.002621  eta: 0h8m  tot: 0h3m29s  (26.8%)34.2%  lr: 0.036547  loss: 0.002620  eta: 0h8m  tot: 0h3m30s  (26.8%)34.8%  lr: 0.036457  loss: 0.002634  eta: 0h8m  tot: 0h3m31s  (27.0%)36.4%  lr: 0.036286  loss: 0.002622  eta: 0h8m  tot: 0h3m34s  (27.3%)%  lr: 0.036166  loss: 0.002635  eta: 0h9m  tot: 0h3m36s  (27.5%)37.7%  lr: 0.036136  loss: 0.002637  eta: 0h9m  tot: 0h3m36s  (27.5%)40.3%  lr: 0.035856  loss: 0.002651  eta: 0h8m  tot: 0h3m40s  (28.1%)40.4%  lr: 0.035846  loss: 0.002654  eta: 0h8m  tot: 0h3m40s  (28.1%)40.6%  lr: 0.035816  loss: 0.002656  eta: 0h8m  tot: 0h3m40s  (28.1%)0.002657  eta: 0h8m  tot: 0h3m41s  (28.2%)41.9%  lr: 0.035676  loss: 0.002666  eta: 0h8m  tot: 0h3m42s  (28.4%)44.5%  lr: 0.035456  loss: 0.002676  eta: 0h8m  tot: 0h3m46s  (28.9%)44.9%  lr: 0.035415  loss: 0.002686  eta: 0h8m  tot: 0h3m47s  (29.0%)45.0%  lr: 0.035405  loss: 0.002686  eta: 0h8m  tot: 0h3m47s  (29.0%)45.5%  lr: 0.035335  loss: 0.002690  eta: 0h8m  tot: 0h3m48s  (29.1%)46.0%  lr: 0.035315  loss: 0.002683  eta: 0h8m  tot: 0h3m49s  (29.2%)46.4%  lr: 0.035305  loss: 0.002685  eta: 0h8m  tot: 0h3m49s  (29.3%)46.5%  lr: 0.035285  loss: 0.002684  eta: 0h8m  tot: 0h3m50s  (29.3%)47.0%  lr: 0.035265  loss: 0.002689  eta: 0h8m  tot: 0h3m51s  (29.4%)47.1%  lr: 0.035245  loss: 0.002688  eta: 0h8m  tot: 0h3m51s  (29.4%)47.9%  lr: 0.035115  loss: 0.002688  eta: 0h8m  tot: 0h3m52s  (29.6%)49.8%  lr: 0.034945  loss: 0.002694  eta: 0h8m  tot: 0h3m55s  (30.0%)50.1%  lr: 0.034895  loss: 0.002699  eta: 0h8m  tot: 0h3m55s  (30.0%)51.9%  lr: 0.034685  loss: 0.002716  eta: 0h8m  tot: 0h3m58s  (30.4%)53.0%  lr: 0.034525  loss: 0.002717  eta: 0h8m  tot: 0h4m0s  (30.6%)53.6%  lr: 0.034505  loss: 0.002714  eta: 0h8m  tot: 0h4m1s  (30.7%)54.4%  lr: 0.034334  loss: 0.002724  eta: 0h8m  tot: 0h4m2s  (30.9%)  lr: 0.034324  loss: 0.002722  eta: 0h8m  tot: 0h4m2s  (30.9%)55.7%  lr: 0.034224  loss: 0.002730  eta: 0h8m  tot: 0h4m4s  (31.1%)57.6%  lr: 0.034014  loss: 0.002739  eta: 0h8m  tot: 0h4m7s  (31.5%)0.034004  loss: 0.002734  eta: 0h8m  tot: 0h4m7s  (31.6%)%  lr: 0.033984  loss: 0.002734  eta: 0h8m  tot: 0h4m7s  (31.6%)58.0%  lr: 0.033974  loss: 0.002735  eta: 0h8m  tot: 0h4m7s  (31.6%)61.8%  lr: 0.033694  loss: 0.002728  eta: 0h8m  tot: 0h4m14s  (32.4%)63.1%  lr: 0.033504  loss: 0.002722  eta: 0h8m  tot: 0h4m16s  (32.6%)64.1%  lr: 0.033363  loss: 0.002719  eta: 0h8m  tot: 0h4m18s  (32.8%)64.2%  lr: 0.033343  loss: 0.002719  eta: 0h8m  tot: 0h4m18s  (32.8%)64.7%  lr: 0.033253  loss: 0.002728  eta: 0h8m  tot: 0h4m18s  (32.9%)65.4%  lr: 0.033143  loss: 0.002722  eta: 0h8m  tot: 0h4m19s  (33.1%)65.9%  lr: 0.033063  loss: 0.002724  eta: 0h8m  tot: 0h4m20s  (33.2%)66.5%  lr: 0.033053  loss: 0.002724  eta: 0h8m  tot: 0h4m21s  (33.3%)67.3%  lr: 0.032983  loss: 0.002720  eta: 0h8m  tot: 0h4m22s  (33.5%)67.7%  lr: 0.032953  loss: 0.002719  eta: 0h8m  tot: 0h4m23s  (33.5%)68.6%  lr: 0.032903  loss: 0.002711  eta: 0h8m  tot: 0h4m24s  (33.7%)69.2%  lr: 0.032893  loss: 0.002701  eta: 0h8m  tot: 0h4m25s  (33.8%)69.8%  lr: 0.032873  loss: 0.002695  eta: 0h8m  tot: 0h4m26s  (34.0%)70.4%  lr: 0.032833  loss: 0.002696  eta: 0h8m  tot: 0h4m27s  (34.1%)71.0%  lr: 0.032823  loss: 0.002685  eta: 0h8m  tot: 0h4m28s  (34.2%)71.1%  lr: 0.032803  loss: 0.002684  eta: 0h8m  tot: 0h4m28s  (34.2%)71.9%  lr: 0.032743  loss: 0.002677  eta: 0h8m  tot: 0h4m29s  (34.4%)75.3%  lr: 0.032372  loss: 0.002663  eta: 0h8m  tot: 0h4m34s  (35.1%)75.7%  lr: 0.032362  loss: 0.002667  eta: 0h8m  tot: 0h4m35s  (35.1%)76.7%  lr: 0.032222  loss: 0.002668  eta: 0h8m  tot: 0h4m36s  (35.3%)%  lr: 0.031932  loss: 0.002666  eta: 0h8m  tot: 0h4m40s  (36.0%)81.1%  lr: 0.031812  loss: 0.002662  eta: 0h8m  tot: 0h4m42s  (36.2%)81.7%  lr: 0.031782  loss: 0.002659  eta: 0h7m  tot: 0h4m43s  (36.3%)82.6%  lr: 0.031682  loss: 0.002654  eta: 0h7m  tot: 0h4m45s  (36.5%)82.8%  lr: 0.031662  loss: 0.002651  eta: 0h7m  tot: 0h4m45s  (36.6%)82.9%  lr: 0.031652  loss: 0.002652  eta: 0h7m  tot: 0h4m45s  (36.6%)84.4%  lr: 0.031552  loss: 0.002649  eta: 0h7m  tot: 0h4m47s  (36.9%)85.1%  lr: 0.031472  loss: 0.002654  eta: 0h7m  tot: 0h4m48s  (37.0%)85.9%  lr: 0.031372  loss: 0.002647  eta: 0h7m  tot: 0h4m49s  (37.2%)87.1%  lr: 0.031231  loss: 0.002650  eta: 0h7m  tot: 0h4m51s  (37.4%)87.4%  lr: 0.031201  loss: 0.002647  eta: 0h7m  tot: 0h4m52s  (37.5%)%  lr: 0.031191  loss: 0.002651  eta: 0h7m  tot: 0h4m52s  (37.6%)88.1%  lr: 0.031171  loss: 0.002653  eta: 0h7m  tot: 0h4m53s  (37.6%)88.5%  lr: 0.031131  loss: 0.002650  eta: 0h7m  tot: 0h4m53s  (37.7%)89.8%  lr: 0.030981  loss: 0.002650  eta: 0h7m  tot: 0h4m56s  (38.0%)90.3%  lr: 0.030941  loss: 0.002652  eta: 0h7m  tot: 0h4m56s  (38.1%)91.6%  lr: 0.030731  loss: 0.002656  eta: 0h7m  tot: 0h4m59s  (38.3%)92.8%  lr: 0.030631  loss: 0.002655  eta: 0h7m  tot: 0h5m0s  (38.6%)0.002655  eta: 0h7m  tot: 0h5m2s  (38.7%)94.4%  lr: 0.030471  loss: 0.002658  eta: 0h7m  tot: 0h5m3s  (38.9%)94.6%  lr: 0.030451  loss: 0.002659  eta: 0h7m  tot: 0h5m3s  (38.9%)94.7%  lr: 0.030451  loss: 0.002659  eta: 0h7m  tot: 0h5m3s  (38.9%)95.9%  lr: 0.030330  loss: 0.002656  eta: 0h7m  tot: 0h5m5s  (39.2%)96.1%  lr: 0.030290  loss: 0.002656  eta: 0h7m  tot: 0h5m5s  (39.2%)97.2%  lr: 0.030180  loss: 0.002652  eta: 0h7m  tot: 0h5m7s  (39.4%)97.9%  lr: 0.030160  loss: 0.002651  eta: 0h7m  tot: 0h5m8s  (39.6%)98.7%  lr: 0.030080  loss: 0.002652  eta: 0h7m  tot: 0h5m9s  (39.7%)\n",
      " ---+++                Epoch    1 Train error : 0.00267407 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001885  eta: 0h5m  tot: 0h7m42s  (60.0%)6%  lr: 0.029950  loss: 0.001822  eta: 0h6m  tot: 0h5m11s  (40.1%)%  lr: 0.029880  loss: 0.002056  eta: 0h6m  tot: 0h5m13s  (40.3%)2.2%  lr: 0.029780  loss: 0.001918  eta: 0h7m  tot: 0h5m14s  (40.4%)3.2%  lr: 0.029620  loss: 0.001986  eta: 0h7m  tot: 0h5m15s  (40.6%)4.1%  lr: 0.029520  loss: 0.001971  eta: 0h7m  tot: 0h5m17s  (40.8%)4.2%  lr: 0.029500  loss: 0.001949  eta: 0h7m  tot: 0h5m17s  (40.8%)4.3%  lr: 0.029469  loss: 0.001952  eta: 0h7m  tot: 0h5m17s  (40.9%)5.2%  lr: 0.029379  loss: 0.001910  eta: 0h7m  tot: 0h5m18s  (41.0%)6.5%  lr: 0.029249  loss: 0.001843  eta: 0h7m  tot: 0h5m20s  (41.3%)6.7%  lr: 0.029229  loss: 0.001863  eta: 0h7m  tot: 0h5m20s  (41.3%)7.1%  lr: 0.029159  loss: 0.001844  eta: 0h7m  tot: 0h5m21s  (41.4%)7m  tot: 0h5m23s  (41.6%)8.6%  lr: 0.029029  loss: 0.001780  eta: 0h7m  tot: 0h5m23s  (41.7%)8.7%  lr: 0.029029  loss: 0.001774  eta: 0h7m  tot: 0h5m24s  (41.7%)9.2%  lr: 0.028989  loss: 0.001814  eta: 0h7m  tot: 0h5m24s  (41.8%)9.7%  lr: 0.028919  loss: 0.001823  eta: 0h7m  tot: 0h5m25s  (41.9%)10.4%  lr: 0.028839  loss: 0.001820  eta: 0h7m  tot: 0h5m26s  (42.1%)10.8%  lr: 0.028809  loss: 0.001820  eta: 0h7m  tot: 0h5m27s  (42.2%)19.0%  lr: 0.027968  loss: 0.001807  eta: 0h7m  tot: 0h5m40s  (43.8%)20.4%  lr: 0.027838  loss: 0.001837  eta: 0h7m  tot: 0h5m42s  (44.1%)22.1%  lr: 0.027658  loss: 0.001865  eta: 0h7m  tot: 0h5m45s  (44.4%)22.5%  lr: 0.027618  loss: 0.001870  eta: 0h7m  tot: 0h5m45s  (44.5%)23.5%  lr: 0.027508  loss: 0.001848  eta: 0h7m  tot: 0h5m47s  (44.7%)%  lr: 0.027478  loss: 0.001851  eta: 0h7m  tot: 0h5m47s  (44.8%)24.6%  lr: 0.027437  loss: 0.001844  eta: 0h7m  tot: 0h5m48s  (44.9%)24.8%  lr: 0.027387  loss: 0.001842  eta: 0h7m  tot: 0h5m49s  (45.0%)25.2%  lr: 0.027367  loss: 0.001834  eta: 0h7m  tot: 0h5m50s  (45.0%)25.8%  lr: 0.027277  loss: 0.001826  eta: 0h7m  tot: 0h5m50s  (45.2%)27.4%  lr: 0.027147  loss: 0.001824  eta: 0h7m  tot: 0h5m53s  (45.5%)28.1%  lr: 0.027117  loss: 0.001822  eta: 0h6m  tot: 0h5m54s  (45.6%)29.7%  lr: 0.027017  loss: 0.001838  eta: 0h6m  tot: 0h5m56s  (45.9%)30.9%  lr: 0.026937  loss: 0.001828  eta: 0h6m  tot: 0h5m57s  (46.2%)33.0%  lr: 0.026727  loss: 0.001844  eta: 0h6m  tot: 0h6m1s  (46.6%)0.026717  loss: 0.001840  eta: 0h6m  tot: 0h6m1s  (46.7%)33.5%  lr: 0.026717  loss: 0.001843  eta: 0h6m  tot: 0h6m1s  (46.7%)33.8%  lr: 0.026707  loss: 0.001844  eta: 0h6m  tot: 0h6m2s  (46.8%)34.0%  lr: 0.026657  loss: 0.001851  eta: 0h6m  tot: 0h6m2s  (46.8%)34.2%  lr: 0.026627  loss: 0.001854  eta: 0h6m  tot: 0h6m2s  (46.8%)%  lr: 0.026557  loss: 0.001854  eta: 0h6m  tot: 0h6m4s  (47.0%)34.9%  lr: 0.026547  loss: 0.001856  eta: 0h6m  tot: 0h6m4s  (47.0%)35.7%  lr: 0.026487  loss: 0.001866  eta: 0h6m  tot: 0h6m5s  (47.1%)%  lr: 0.026487  loss: 0.001864  eta: 0h6m  tot: 0h6m5s  (47.2%)35.9%  lr: 0.026477  loss: 0.001866  eta: 0h6m  tot: 0h6m5s  (47.2%)37.4%  lr: 0.026276  loss: 0.001885  eta: 0h6m  tot: 0h6m8s  (47.5%)38.2%  lr: 0.026206  loss: 0.001880  eta: 0h6m  tot: 0h6m9s  (47.6%)38.3%  lr: 0.026196  loss: 0.001883  eta: 0h6m  tot: 0h6m9s  (47.7%)39.4%  lr: 0.026116  loss: 0.001890  eta: 0h6m  tot: 0h6m11s  (47.9%)0.001886  eta: 0h6m  tot: 0h6m12s  (48.1%)41.0%  lr: 0.025956  loss: 0.001881  eta: 0h6m  tot: 0h6m13s  (48.2%)%  lr: 0.025806  loss: 0.001875  eta: 0h6m  tot: 0h6m14s  (48.5%)43.4%  lr: 0.025646  loss: 0.001881  eta: 0h6m  tot: 0h6m16s  (48.7%)44.9%  lr: 0.025536  loss: 0.001873  eta: 0h6m  tot: 0h6m18s  (49.0%)46.8%  lr: 0.025315  loss: 0.001867  eta: 0h6m  tot: 0h6m21s  (49.4%)0.001873  eta: 0h6m  tot: 0h6m21s  (49.4%)47.9%  lr: 0.025235  loss: 0.001868  eta: 0h6m  tot: 0h6m23s  (49.6%)48.5%  lr: 0.025155  loss: 0.001862  eta: 0h6m  tot: 0h6m24s  (49.7%)49.9%  lr: 0.024965  loss: 0.001858  eta: 0h6m  tot: 0h6m26s  (50.0%)51.3%  lr: 0.024855  loss: 0.001857  eta: 0h6m  tot: 0h6m28s  (50.3%)51.5%  lr: 0.024825  loss: 0.001857  eta: 0h6m  tot: 0h6m28s  (50.3%)51.9%  lr: 0.024815  loss: 0.001857  eta: 0h6m  tot: 0h6m29s  (50.4%)54.3%  lr: 0.024665  loss: 0.001858  eta: 0h6m  tot: 0h6m33s  (50.9%)54.7%  lr: 0.024615  loss: 0.001865  eta: 0h6m  tot: 0h6m33s  (50.9%)55.0%  lr: 0.024585  loss: 0.001869  eta: 0h6m  tot: 0h6m34s  (51.0%)55.6%  lr: 0.024515  loss: 0.001873  eta: 0h6m  tot: 0h6m35s  (51.1%)%  lr: 0.024475  loss: 0.001871  eta: 0h6m  tot: 0h6m36s  (51.3%)  tot: 0h6m38s  (51.5%)57.6%  lr: 0.024304  loss: 0.001880  eta: 0h6m  tot: 0h6m38s  (51.5%)58.1%  lr: 0.024274  loss: 0.001879  eta: 0h6m  tot: 0h6m39s  (51.6%)58.4%  lr: 0.024264  loss: 0.001883  eta: 0h6m  tot: 0h6m39s  (51.7%)60.8%  lr: 0.024034  loss: 0.001888  eta: 0h6m  tot: 0h6m43s  (52.2%)%  lr: 0.023894  loss: 0.001888  eta: 0h6m  tot: 0h6m45s  (52.4%)64.2%  lr: 0.023614  loss: 0.001891  eta: 0h6m  tot: 0h6m49s  (52.8%)64.9%  lr: 0.023484  loss: 0.001898  eta: 0h5m  tot: 0h6m50s  (53.0%)65.0%  lr: 0.023484  loss: 0.001898  eta: 0h5m  tot: 0h6m50s  (53.0%)65.1%  lr: 0.023464  loss: 0.001898  eta: 0h5m  tot: 0h6m50s  (53.0%)  lr: 0.023434  loss: 0.001894  eta: 0h5m  tot: 0h6m51s  (53.1%)65.8%  lr: 0.023393  loss: 0.001893  eta: 0h5m  tot: 0h6m51s  (53.2%)66.7%  lr: 0.023283  loss: 0.001886  eta: 0h5m  tot: 0h6m53s  (53.3%)67.2%  lr: 0.023233  loss: 0.001889  eta: 0h5m  tot: 0h6m54s  (53.4%)67.5%  lr: 0.023213  loss: 0.001889  eta: 0h5m  tot: 0h6m54s  (53.5%)67.6%  lr: 0.023203  loss: 0.001890  eta: 0h5m  tot: 0h6m54s  (53.5%)68.1%  lr: 0.023173  loss: 0.001889  eta: 0h5m  tot: 0h6m55s  (53.6%)69.7%  lr: 0.023033  loss: 0.001884  eta: 0h5m  tot: 0h6m57s  (53.9%)69.9%  lr: 0.022993  loss: 0.001883  eta: 0h5m  tot: 0h6m58s  (54.0%)70.1%  lr: 0.022973  loss: 0.001880  eta: 0h5m  tot: 0h6m58s  (54.0%)71.4%  lr: 0.022803  loss: 0.001888  eta: 0h5m  tot: 0h7m0s  (54.3%)71.7%  lr: 0.022793  loss: 0.001888  eta: 0h5m  tot: 0h7m0s  (54.3%)72.4%  lr: 0.022753  loss: 0.001888  eta: 0h5m  tot: 0h7m1s  (54.5%)0.001892  eta: 0h5m  tot: 0h7m2s  (54.6%)77.3%  lr: 0.022312  loss: 0.001900  eta: 0h5m  tot: 0h7m9s  (55.5%)78.5%  lr: 0.022262  loss: 0.001898  eta: 0h5m  tot: 0h7m10s  (55.7%)79.6%  lr: 0.022142  loss: 0.001891  eta: 0h5m  tot: 0h7m12s  (55.9%)79.9%  lr: 0.022082  loss: 0.001890  eta: 0h5m  tot: 0h7m13s  (56.0%)7m14s  (56.1%)81.8%  lr: 0.021992  loss: 0.001894  eta: 0h5m  tot: 0h7m16s  (56.4%)83.4%  lr: 0.021812  loss: 0.001898  eta: 0h5m  tot: 0h7m18s  (56.7%)83.5%  lr: 0.021792  loss: 0.001896  eta: 0h5m  tot: 0h7m18s  (56.7%)85.0%  lr: 0.021712  loss: 0.001901  eta: 0h5m  tot: 0h7m21s  (57.0%)86.8%  lr: 0.021492  loss: 0.001894  eta: 0h5m  tot: 0h7m23s  (57.4%)%  lr: 0.021231  loss: 0.001883  eta: 0h5m  tot: 0h7m26s  (57.8%)89.0%  lr: 0.021201  loss: 0.001884  eta: 0h5m  tot: 0h7m27s  (57.8%)89.1%  lr: 0.021161  loss: 0.001884  eta: 0h5m  tot: 0h7m27s  (57.8%)89.5%  lr: 0.021121  loss: 0.001884  eta: 0h5m  tot: 0h7m27s  (57.9%)89.6%  lr: 0.021121  loss: 0.001883  eta: 0h5m  tot: 0h7m28s  (57.9%)90.3%  lr: 0.021051  loss: 0.001887  eta: 0h5m  tot: 0h7m29s  (58.1%)90.9%  lr: 0.020921  loss: 0.001889  eta: 0h5m  tot: 0h7m30s  (58.2%)91.7%  lr: 0.020781  loss: 0.001889  eta: 0h5m  tot: 0h7m31s  (58.3%)93.2%  lr: 0.020671  loss: 0.001886  eta: 0h5m  tot: 0h7m33s  (58.6%)94.3%  lr: 0.020491  loss: 0.001892  eta: 0h5m  tot: 0h7m35s  (58.9%)97.2%  lr: 0.020120  loss: 0.001887  eta: 0h5m  tot: 0h7m40s  (59.4%)\n",
      " ---+++                Epoch    2 Train error : 0.00190784 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91.9%  lr: 0.010551  loss: 0.001617  eta: 0h2m  tot: 0h10m7s  (78.4%).8%  lr: 0.019960  loss: 0.001884  eta: 0h3m  tot: 0h7m43s  (60.2%)2.0%  lr: 0.019840  loss: 0.001718  eta: 0h4m  tot: 0h7m45s  (60.4%)2.2%  lr: 0.019810  loss: 0.001805  eta: 0h4m  tot: 0h7m46s  (60.4%)3.0%  lr: 0.019720  loss: 0.001739  eta: 0h5m  tot: 0h7m47s  (60.6%)3.9%  lr: 0.019620  loss: 0.001667  eta: 0h5m  tot: 0h7m49s  (60.8%)4.7%  lr: 0.019560  loss: 0.001583  eta: 0h5m  tot: 0h7m50s  (60.9%)%  lr: 0.019560  loss: 0.001584  eta: 0h5m  tot: 0h7m50s  (61.0%)5.1%  lr: 0.019540  loss: 0.001567  eta: 0h5m  tot: 0h7m51s  (61.0%)%  lr: 0.019510  loss: 0.001590  eta: 0h5m  tot: 0h7m51s  (61.1%)5.5%  lr: 0.019479  loss: 0.001584  eta: 0h5m  tot: 0h7m51s  (61.1%)5.9%  lr: 0.019399  loss: 0.001588  eta: 0h5m  tot: 0h7m52s  (61.2%)7.4%  lr: 0.019199  loss: 0.001625  eta: 0h5m  tot: 0h7m54s  (61.5%)8.4%  lr: 0.019109  loss: 0.001612  eta: 0h5m  tot: 0h7m56s  (61.7%)10.3%  lr: 0.018929  loss: 0.001622  eta: 0h5m  tot: 0h7m59s  (62.1%)10.4%  lr: 0.018889  loss: 0.001618  eta: 0h5m  tot: 0h7m59s  (62.1%)10.5%  lr: 0.018879  loss: 0.001627  eta: 0h5m  tot: 0h7m59s  (62.1%)11.5%  lr: 0.018829  loss: 0.001606  eta: 0h4m  tot: 0h8m0s  (62.3%)12.2%  lr: 0.018759  loss: 0.001618  eta: 0h4m  tot: 0h8m2s  (62.4%)12.7%  lr: 0.018699  loss: 0.001624  eta: 0h4m  tot: 0h8m2s  (62.5%)14.5%  lr: 0.018489  loss: 0.001623  eta: 0h4m  tot: 0h8m5s  (62.9%)14.8%  lr: 0.018458  loss: 0.001630  eta: 0h4m  tot: 0h8m5s  (63.0%)17.5%  lr: 0.018218  loss: 0.001629  eta: 0h4m  tot: 0h8m10s  (63.5%)18.0%  lr: 0.018168  loss: 0.001644  eta: 0h4m  tot: 0h8m10s  (63.6%)18.4%  lr: 0.018168  loss: 0.001637  eta: 0h4m  tot: 0h8m11s  (63.7%)18.5%  lr: 0.018168  loss: 0.001632  eta: 0h4m  tot: 0h8m11s  (63.7%)20.0%  lr: 0.018088  loss: 0.001610  eta: 0h4m  tot: 0h8m13s  (64.0%)20.2%  lr: 0.018078  loss: 0.001613  eta: 0h4m  tot: 0h8m13s  (64.0%)20.3%  lr: 0.018078  loss: 0.001619  eta: 0h4m  tot: 0h8m13s  (64.1%)24.8%  lr: 0.017738  loss: 0.001583  eta: 0h4m  tot: 0h8m21s  (65.0%)25.5%  lr: 0.017618  loss: 0.001595  eta: 0h4m  tot: 0h8m22s  (65.1%)25.8%  lr: 0.017568  loss: 0.001604  eta: 0h4m  tot: 0h8m22s  (65.2%)%  lr: 0.017568  loss: 0.001605  eta: 0h4m  tot: 0h8m22s  (65.2%)28.2%  lr: 0.017367  loss: 0.001597  eta: 0h4m  tot: 0h8m26s  (65.6%)30.3%  lr: 0.017157  loss: 0.001587  eta: 0h4m  tot: 0h8m29s  (66.1%)30.8%  lr: 0.017067  loss: 0.001585  eta: 0h4m  tot: 0h8m30s  (66.2%)32.2%  lr: 0.016867  loss: 0.001587  eta: 0h4m  tot: 0h8m32s  (66.4%)33.8%  lr: 0.016717  loss: 0.001606  eta: 0h4m  tot: 0h8m35s  (66.8%)%  lr: 0.016717  loss: 0.001607  eta: 0h4m  tot: 0h8m35s  (66.8%)34.5%  lr: 0.016707  loss: 0.001606  eta: 0h4m  tot: 0h8m36s  (66.9%)35.8%  lr: 0.016517  loss: 0.001597  eta: 0h4m  tot: 0h8m39s  (67.2%)37.4%  lr: 0.016296  loss: 0.001614  eta: 0h4m  tot: 0h8m41s  (67.5%)38.1%  lr: 0.016246  loss: 0.001617  eta: 0h4m  tot: 0h8m42s  (67.6%)39.5%  lr: 0.016116  loss: 0.001611  eta: 0h4m  tot: 0h8m45s  (67.9%)39.6%  lr: 0.016116  loss: 0.001615  eta: 0h4m  tot: 0h8m45s  (67.9%)39.7%  lr: 0.016096  loss: 0.001614  eta: 0h4m  tot: 0h8m45s  (67.9%)%  lr: 0.016056  loss: 0.001616  eta: 0h4m  tot: 0h8m45s  (68.0%)41.0%  lr: 0.015966  loss: 0.001608  eta: 0h4m  tot: 0h8m47s  (68.2%)41.6%  lr: 0.015946  loss: 0.001608  eta: 0h4m  tot: 0h8m48s  (68.3%)41.8%  lr: 0.015946  loss: 0.001610  eta: 0h4m  tot: 0h8m48s  (68.4%)42.3%  lr: 0.015876  loss: 0.001603  eta: 0h4m  tot: 0h8m49s  (68.5%)0.001601  eta: 0h4m  tot: 0h8m49s  (68.6%)%  lr: 0.015726  loss: 0.001599  eta: 0h4m  tot: 0h8m51s  (68.8%)44.0%  lr: 0.015716  loss: 0.001599  eta: 0h4m  tot: 0h8m51s  (68.8%)%  lr: 0.015706  loss: 0.001607  eta: 0h4m  tot: 0h8m52s  (68.8%)44.3%  lr: 0.015696  loss: 0.001607  eta: 0h4m  tot: 0h8m52s  (68.9%)45.0%  lr: 0.015606  loss: 0.001613  eta: 0h4m  tot: 0h8m53s  (69.0%)46.4%  lr: 0.015425  loss: 0.001617  eta: 0h4m  tot: 0h8m55s  (69.3%)46.5%  lr: 0.015405  loss: 0.001617  eta: 0h4m  tot: 0h8m55s  (69.3%)46.8%  lr: 0.015395  loss: 0.001619  eta: 0h4m  tot: 0h8m56s  (69.4%)48.2%  lr: 0.015165  loss: 0.001607  eta: 0h4m  tot: 0h8m59s  (69.6%)48.7%  lr: 0.015075  loss: 0.001603  eta: 0h3m  tot: 0h8m59s  (69.7%)%  lr: 0.015015  loss: 0.001598  eta: 0h3m  tot: 0h9m1s  (69.9%)50.4%  lr: 0.014925  loss: 0.001602  eta: 0h3m  tot: 0h9m2s  (70.1%)%  lr: 0.014925  loss: 0.001599  eta: 0h3m  tot: 0h9m2s  (70.1%)51.0%  lr: 0.014845  loss: 0.001597  eta: 0h3m  tot: 0h9m3s  (70.2%)51.3%  lr: 0.014825  loss: 0.001597  eta: 0h3m  tot: 0h9m3s  (70.3%)51.8%  lr: 0.014765  loss: 0.001601  eta: 0h3m  tot: 0h9m4s  (70.4%)53.8%  lr: 0.014565  loss: 0.001597  eta: 0h3m  tot: 0h9m8s  (70.8%)54.0%  lr: 0.014535  loss: 0.001602  eta: 0h3m  tot: 0h9m8s  (70.8%)%  lr: 0.014525  loss: 0.001600  eta: 0h3m  tot: 0h9m9s  (70.9%)55.4%  lr: 0.014425  loss: 0.001600  eta: 0h3m  tot: 0h9m10s  (71.1%)56.2%  lr: 0.014364  loss: 0.001601  eta: 0h3m  tot: 0h9m12s  (71.2%)56.3%  lr: 0.014364  loss: 0.001600  eta: 0h3m  tot: 0h9m12s  (71.3%)58.0%  lr: 0.014184  loss: 0.001607  eta: 0h3m  tot: 0h9m15s  (71.6%)0h3m  tot: 0h9m17s  (71.9%)59.4%  lr: 0.014044  loss: 0.001606  eta: 0h3m  tot: 0h9m17s  (71.9%)59.5%  lr: 0.014034  loss: 0.001605  eta: 0h3m  tot: 0h9m17s  (71.9%)60.8%  lr: 0.013894  loss: 0.001599  eta: 0h3m  tot: 0h9m19s  (72.2%)%  lr: 0.013884  loss: 0.001599  eta: 0h3m  tot: 0h9m19s  (72.2%)62.5%  lr: 0.013734  loss: 0.001597  eta: 0h3m  tot: 0h9m22s  (72.5%)62.8%  lr: 0.013704  loss: 0.001597  eta: 0h3m  tot: 0h9m23s  (72.6%)63.2%  lr: 0.013664  loss: 0.001602  eta: 0h3m  tot: 0h9m23s  (72.6%)64.6%  lr: 0.013504  loss: 0.001605  eta: 0h3m  tot: 0h9m26s  (72.9%)64.7%  lr: 0.013474  loss: 0.001604  eta: 0h3m  tot: 0h9m26s  (72.9%)66.8%  lr: 0.013143  loss: 0.001612  eta: 0h3m  tot: 0h9m29s  (73.4%)68.8%  lr: 0.012933  loss: 0.001608  eta: 0h3m  tot: 0h9m32s  (73.8%)68.9%  lr: 0.012903  loss: 0.001609  eta: 0h3m  tot: 0h9m32s  (73.8%)69.0%  lr: 0.012903  loss: 0.001609  eta: 0h3m  tot: 0h9m33s  (73.8%)69.9%  lr: 0.012783  loss: 0.001607  eta: 0h3m  tot: 0h9m34s  (74.0%)70.3%  lr: 0.012753  loss: 0.001606  eta: 0h3m  tot: 0h9m35s  (74.1%)71.3%  lr: 0.012643  loss: 0.001607  eta: 0h3m  tot: 0h9m36s  (74.3%)71.9%  lr: 0.012593  loss: 0.001607  eta: 0h3m  tot: 0h9m37s  (74.4%)%  lr: 0.012563  loss: 0.001604  eta: 0h3m  tot: 0h9m37s  (74.4%)72.6%  lr: 0.012493  loss: 0.001605  eta: 0h3m  tot: 0h9m38s  (74.5%)  lr: 0.012423  loss: 0.001603  eta: 0h3m  tot: 0h9m39s  (74.6%)74.8%  lr: 0.012232  loss: 0.001606  eta: 0h3m  tot: 0h9m41s  (75.0%)76.2%  lr: 0.012162  loss: 0.001608  eta: 0h3m  tot: 0h9m43s  (75.2%)77.9%  lr: 0.011982  loss: 0.001605  eta: 0h3m  tot: 0h9m46s  (75.6%)78.0%  lr: 0.011982  loss: 0.001606  eta: 0h3m  tot: 0h9m46s  (75.6%)79.1%  lr: 0.011902  loss: 0.001614  eta: 0h3m  tot: 0h9m48s  (75.8%)79.5%  lr: 0.011842  loss: 0.001618  eta: 0h3m  tot: 0h9m48s  (75.9%)79.6%  lr: 0.011842  loss: 0.001619  eta: 0h3m  tot: 0h9m48s  (75.9%)82.7%  lr: 0.011662  loss: 0.001617  eta: 0h3m  tot: 0h9m53s  (76.5%)84.7%  lr: 0.011432  loss: 0.001619  eta: 0h3m  tot: 0h9m56s  (76.9%)85.3%  lr: 0.011361  loss: 0.001623  eta: 0h3m  tot: 0h9m57s  (77.1%)85.4%  lr: 0.011341  loss: 0.001622  eta: 0h3m  tot: 0h9m57s  (77.1%)85.8%  lr: 0.011311  loss: 0.001621  eta: 0h3m  tot: 0h9m58s  (77.2%)88.2%  lr: 0.011051  loss: 0.001623  eta: 0h2m  tot: 0h10m1s  (77.6%)88.4%  lr: 0.011021  loss: 0.001621  eta: 0h2m  tot: 0h10m1s  (77.7%)88.8%  lr: 0.011001  loss: 0.001621  eta: 0h2m  tot: 0h10m2s  (77.8%)%  lr: 0.010931  loss: 0.001623  eta: 0h2m  tot: 0h10m3s  (77.9%)89.5%  lr: 0.010881  loss: 0.001621  eta: 0h2m  tot: 0h10m3s  (77.9%)89.6%  lr: 0.010871  loss: 0.001621  eta: 0h2m  tot: 0h10m3s  (77.9%)89.7%  lr: 0.010861  loss: 0.001621  eta: 0h2m  tot: 0h10m3s  (77.9%)89.8%  lr: 0.010841  loss: 0.001620  eta: 0h2m  tot: 0h10m4s  (78.0%)90.0%  lr: 0.010811  loss: 0.001621  eta: 0h2m  tot: 0h10m4s  (78.0%)90.9%  lr: 0.010691  loss: 0.001618  eta: 0h2m  tot: 0h10m5s  (78.2%)91.2%  lr: 0.010651  loss: 0.001618  eta: 0h2m  tot: 0h10m6s  (78.2%)91.8%  lr: 0.010561  loss: 0.001618  eta: 0h2m  tot: 0h10m7s  (78.4%)92.0%  lr: 0.010551  loss: 0.001621  eta: 0h2m  tot: 0h10m7s  (78.4%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001609  eta: 0h2m  tot: 0h10m17s  (80.0%).6%  lr: 0.010401  loss: 0.001621  eta: 0h2m  tot: 0h10m9s  (78.7%)94.4%  lr: 0.010320  loss: 0.001619  eta: 0h2m  tot: 0h10m11s  (78.9%)%  lr: 0.010290  loss: 0.001619  eta: 0h2m  tot: 0h10m11s  (78.9%)95.0%  lr: 0.010270  loss: 0.001618  eta: 0h2m  tot: 0h10m12s  (79.0%)96.3%  lr: 0.010190  loss: 0.001614  eta: 0h2m  tot: 0h10m13s  (79.3%)96.8%  lr: 0.010140  loss: 0.001612  eta: 0h2m  tot: 0h10m14s  (79.4%)96.9%  lr: 0.010140  loss: 0.001611  eta: 0h2m  tot: 0h10m14s  (79.4%)\n",
      " ---+++                Epoch    3 Train error : 0.00155205 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94.0%  lr: 0.000431  loss: 0.001405  eta: <1min   tot: 0h12m42s  (98.8%)  lr: 0.009990  loss: 0.001398  eta: 0h1m  tot: 0h10m17s  (80.1%)0.6%  lr: 0.009970  loss: 0.001486  eta: 0h1m  tot: 0h10m17s  (80.1%)1.2%  lr: 0.009870  loss: 0.001497  eta: 0h2m  tot: 0h10m19s  (80.2%)%  lr: 0.009870  loss: 0.001457  eta: 0h2m  tot: 0h10m19s  (80.3%)1.3%  lr: 0.009860  loss: 0.001538  eta: 0h2m  tot: 0h10m19s  (80.3%)2.5%  lr: 0.009720  loss: 0.001427  eta: 0h2m  tot: 0h10m21s  (80.5%)3.0%  lr: 0.009620  loss: 0.001434  eta: 0h2m  tot: 0h10m21s  (80.6%)3.4%  lr: 0.009540  loss: 0.001421  eta: 0h2m  tot: 0h10m22s  (80.7%)3.8%  lr: 0.009479  loss: 0.001406  eta: 0h2m  tot: 0h10m23s  (80.8%)4.6%  lr: 0.009419  loss: 0.001421  eta: 0h2m  tot: 0h10m24s  (80.9%)4.7%  lr: 0.009399  loss: 0.001420  eta: 0h2m  tot: 0h10m24s  (80.9%)5.7%  lr: 0.009319  loss: 0.001428  eta: 0h2m  tot: 0h10m26s  (81.1%)6.4%  lr: 0.009209  loss: 0.001430  eta: 0h2m  tot: 0h10m27s  (81.3%)6.6%  lr: 0.009189  loss: 0.001435  eta: 0h2m  tot: 0h10m28s  (81.3%)7.0%  lr: 0.009159  loss: 0.001451  eta: 0h2m  tot: 0h10m29s  (81.4%)7.4%  lr: 0.009139  loss: 0.001415  eta: 0h2m  tot: 0h10m29s  (81.5%)7.7%  lr: 0.009119  loss: 0.001397  eta: 0h2m  tot: 0h10m30s  (81.5%)7.7%  lr: 0.009119  loss: 0.001396  eta: 0h2m  tot: 0h10m30s  (81.5%)8.2%  lr: 0.009079  loss: 0.001426  eta: 0h2m  tot: 0h10m30s  (81.6%)8.9%  lr: 0.009009  loss: 0.001438  eta: 0h2m  tot: 0h10m32s  (81.8%)9.0%  lr: 0.008999  loss: 0.001440  eta: 0h2m  tot: 0h10m32s  (81.8%)9.7%  lr: 0.008929  loss: 0.001480  eta: 0h2m  tot: 0h10m33s  (81.9%)11.1%  lr: 0.008839  loss: 0.001442  eta: 0h2m  tot: 0h10m35s  (82.2%)11.3%  lr: 0.008839  loss: 0.001452  eta: 0h2m  tot: 0h10m36s  (82.3%)12.3%  lr: 0.008759  loss: 0.001455  eta: 0h2m  tot: 0h10m37s  (82.5%)17.5%  lr: 0.008128  loss: 0.001424  eta: 0h2m  tot: 0h10m45s  (83.5%)17.6%  lr: 0.008118  loss: 0.001423  eta: 0h2m  tot: 0h10m45s  (83.5%)18.1%  lr: 0.008048  loss: 0.001412  eta: 0h2m  tot: 0h10m46s  (83.6%)21.3%  lr: 0.007738  loss: 0.001387  eta: 0h2m  tot: 0h10m51s  (84.3%)0.007698  loss: 0.001385  eta: 0h2m  tot: 0h10m52s  (84.3%)21.7%  lr: 0.007698  loss: 0.001387  eta: 0h2m  tot: 0h10m52s  (84.3%)%  lr: 0.007518  loss: 0.001393  eta: 0h2m  tot: 0h10m54s  (84.7%)23.8%  lr: 0.007468  loss: 0.001386  eta: 0h2m  tot: 0h10m55s  (84.8%)23.9%  lr: 0.007468  loss: 0.001388  eta: 0h2m  tot: 0h10m55s  (84.8%)%  lr: 0.007397  loss: 0.001378  eta: 0h2m  tot: 0h10m56s  (84.8%)24.5%  lr: 0.007367  loss: 0.001374  eta: 0h2m  tot: 0h10m56s  (84.9%)25.5%  lr: 0.007337  loss: 0.001378  eta: 0h2m  tot: 0h10m58s  (85.1%)25.6%  lr: 0.007317  loss: 0.001388  eta: 0h2m  tot: 0h10m58s  (85.1%)27.5%  lr: 0.007167  loss: 0.001398  eta: 0h1m  tot: 0h11m1s  (85.5%)27.5%  lr: 0.007167  loss: 0.001397  eta: 0h1m  tot: 0h11m1s  (85.5%)27.7%  lr: 0.007157  loss: 0.001392  eta: 0h1m  tot: 0h11m1s  (85.5%)28.7%  lr: 0.007067  loss: 0.001385  eta: 0h1m  tot: 0h11m3s  (85.7%)30.1%  lr: 0.006917  loss: 0.001387  eta: 0h1m  tot: 0h11m5s  (86.0%)30.2%  lr: 0.006907  loss: 0.001384  eta: 0h1m  tot: 0h11m5s  (86.0%)30.5%  lr: 0.006867  loss: 0.001386  eta: 0h1m  tot: 0h11m5s  (86.1%)32.3%  lr: 0.006697  loss: 0.001377  eta: 0h1m  tot: 0h11m8s  (86.5%)33.0%  lr: 0.006627  loss: 0.001385  eta: 0h1m  tot: 0h11m9s  (86.6%)33.6%  lr: 0.006517  loss: 0.001388  eta: 0h1m  tot: 0h11m10s  (86.7%)35.9%  lr: 0.006246  loss: 0.001392  eta: 0h1m  tot: 0h11m14s  (87.2%)36.9%  lr: 0.006186  loss: 0.001385  eta: 0h1m  tot: 0h11m15s  (87.4%)37.4%  lr: 0.006136  loss: 0.001382  eta: 0h1m  tot: 0h11m16s  (87.5%)37.9%  lr: 0.006066  loss: 0.001381  eta: 0h1m  tot: 0h11m17s  (87.6%)38.0%  lr: 0.006056  loss: 0.001382  eta: 0h1m  tot: 0h11m17s  (87.6%)38.5%  lr: 0.006016  loss: 0.001386  eta: 0h1m  tot: 0h11m18s  (87.7%)39.0%  lr: 0.005986  loss: 0.001389  eta: 0h1m  tot: 0h11m18s  (87.8%)40.8%  lr: 0.005846  loss: 0.001383  eta: 0h1m  tot: 0h11m21s  (88.2%)41.5%  lr: 0.005766  loss: 0.001384  eta: 0h1m  tot: 0h11m21s  (88.3%)43.1%  lr: 0.005636  loss: 0.001387  eta: 0h1m  tot: 0h11m24s  (88.6%)%  lr: 0.005546  loss: 0.001389  eta: 0h1m  tot: 0h11m25s  (88.8%)44.5%  lr: 0.005456  loss: 0.001385  eta: 0h1m  tot: 0h11m26s  (88.9%)45.6%  lr: 0.005385  loss: 0.001390  eta: 0h1m  tot: 0h11m28s  (89.1%)48.3%  lr: 0.005015  loss: 0.001407  eta: 0h1m  tot: 0h11m32s  (89.7%)49.2%  lr: 0.004895  loss: 0.001403  eta: 0h1m  tot: 0h11m33s  (89.8%)50.2%  lr: 0.004815  loss: 0.001406  eta: 0h1m  tot: 0h11m35s  (90.0%)51.5%  lr: 0.004675  loss: 0.001407  eta: 0h1m  tot: 0h11m37s  (90.3%)51.7%  lr: 0.004675  loss: 0.001407  eta: 0h1m  tot: 0h11m37s  (90.3%)51.8%  lr: 0.004665  loss: 0.001409  eta: 0h1m  tot: 0h11m37s  (90.4%)52.5%  lr: 0.004615  loss: 0.001404  eta: 0h1m  tot: 0h11m39s  (90.5%)53.3%  lr: 0.004565  loss: 0.001399  eta: 0h1m  tot: 0h11m40s  (90.7%)54.1%  lr: 0.004475  loss: 0.001401  eta: 0h1m  tot: 0h11m41s  (90.8%)57.7%  lr: 0.004054  loss: 0.001410  eta: 0h1m  tot: 0h11m47s  (91.5%)57.9%  lr: 0.004034  loss: 0.001410  eta: 0h1m  tot: 0h11m48s  (91.6%)58.2%  lr: 0.003994  loss: 0.001409  eta: 0h1m  tot: 0h11m48s  (91.6%)58.5%  lr: 0.003964  loss: 0.001411  eta: 0h1m  tot: 0h11m49s  (91.7%)58.9%  lr: 0.003944  loss: 0.001408  eta: 0h1m  tot: 0h11m49s  (91.8%)59.7%  lr: 0.003854  loss: 0.001402  eta: 0h1m  tot: 0h11m50s  (91.9%)%  lr: 0.003794  loss: 0.001401  eta: 0h1m  tot: 0h11m51s  (92.0%)65.1%  lr: 0.003323  loss: 0.001407  eta: <1min   tot: 0h11m58s  (93.0%)65.4%  lr: 0.003283  loss: 0.001408  eta: <1min   tot: 0h11m59s  (93.1%)66.2%  lr: 0.003233  loss: 0.001410  eta: <1min   tot: 0h12m0s  (93.2%)68.1%  lr: 0.003093  loss: 0.001408  eta: <1min   tot: 0h12m2s  (93.6%)68.3%  lr: 0.003053  loss: 0.001411  eta: <1min   tot: 0h12m3s  (93.7%)69.2%  lr: 0.002943  loss: 0.001416  eta: <1min   tot: 0h12m4s  (93.8%)69.9%  lr: 0.002843  loss: 0.001416  eta: <1min   tot: 0h12m6s  (94.0%)71.0%  lr: 0.002733  loss: 0.001419  eta: <1min   tot: 0h12m7s  (94.2%)71.4%  lr: 0.002663  loss: 0.001418  eta: <1min   tot: 0h12m8s  (94.3%)72.1%  lr: 0.002593  loss: 0.001417  eta: <1min   tot: 0h12m9s  (94.4%)73.8%  lr: 0.002453  loss: 0.001416  eta: <1min   tot: 0h12m11s  (94.8%)75.4%  lr: 0.002362  loss: 0.001414  eta: <1min   tot: 0h12m14s  (95.1%)75.5%  lr: 0.002342  loss: 0.001413  eta: <1min   tot: 0h12m14s  (95.1%)75.6%  lr: 0.002342  loss: 0.001414  eta: <1min   tot: 0h12m14s  (95.1%)75.7%  lr: 0.002342  loss: 0.001413  eta: <1min   tot: 0h12m14s  (95.1%)75.8%  lr: 0.002322  loss: 0.001413  eta: <1min   tot: 0h12m14s  (95.2%)76.0%  lr: 0.002292  loss: 0.001411  eta: <1min   tot: 0h12m14s  (95.2%)76.2%  lr: 0.002262  loss: 0.001413  eta: <1min   tot: 0h12m15s  (95.2%)77.1%  lr: 0.002172  loss: 0.001415  eta: <1min   tot: 0h12m16s  (95.4%)77.9%  lr: 0.002102  loss: 0.001411  eta: <1min   tot: 0h12m18s  (95.6%)78.0%  lr: 0.002102  loss: 0.001411  eta: <1min   tot: 0h12m18s  (95.6%)78.2%  lr: 0.002092  loss: 0.001409  eta: <1min   tot: 0h12m18s  (95.6%)79.2%  lr: 0.001972  loss: 0.001410  eta: <1min   tot: 0h12m19s  (95.8%)79.4%  lr: 0.001952  loss: 0.001409  eta: <1min   tot: 0h12m20s  (95.9%)80.0%  lr: 0.001902  loss: 0.001411  eta: <1min   tot: 0h12m21s  (96.0%)80.6%  lr: 0.001802  loss: 0.001408  eta: <1min   tot: 0h12m21s  (96.1%)81.6%  lr: 0.001682  loss: 0.001409  eta: <1min   tot: 0h12m23s  (96.3%)81.8%  lr: 0.001632  loss: 0.001410  eta: <1min   tot: 0h12m23s  (96.4%)84.3%  lr: 0.001371  loss: 0.001408  eta: <1min   tot: 0h12m27s  (96.9%)87.9%  lr: 0.001111  loss: 0.001398  eta: <1min   tot: 0h12m33s  (97.6%)88.7%  lr: 0.001031  loss: 0.001398  eta: <1min   tot: 0h12m34s  (97.7%)%  lr: 0.001021  loss: 0.001398  eta: <1min   tot: 0h12m34s  (97.8%)90.0%  lr: 0.000901  loss: 0.001399  eta: <1min   tot: 0h12m36s  (98.0%)90.5%  lr: 0.000831  loss: 0.001402  eta: <1min   tot: 0h12m37s  (98.1%)91.1%  lr: 0.000791  loss: 0.001403  eta: <1min   tot: 0h12m38s  (98.2%)91.7%  lr: 0.000711  loss: 0.001405  eta: <1min   tot: 0h12m38s  (98.3%)93.0%  lr: 0.000531  loss: 0.001405  eta: <1min   tot: 0h12m40s  (98.6%)93.2%  lr: 0.000511  loss: 0.001406  eta: <1min   tot: 0h12m41s  (98.6%)94.1%  lr: 0.000410  loss: 0.001404  eta: <1min   tot: 0h12m42s  (98.8%)Epoch: 100.0%  lr: 0.000000  loss: 0.001393  eta: <1min   tot: 0h12m49s  (100.0%).7%  lr: 0.000290  loss: 0.001401  eta: <1min   tot: 0h12m43s  (98.9%)95.5%  lr: 0.000180  loss: 0.001401  eta: <1min   tot: 0h12m45s  (99.1%)96.5%  lr: 0.000120  loss: 0.001398  eta: <1min   tot: 0h12m46s  (99.3%)97.4%  lr: 0.000070  loss: 0.001398  eta: <1min   tot: 0h12m47s  (99.5%)97.5%  lr: 0.000060  loss: 0.001399  eta: <1min   tot: 0h12m47s  (99.5%)97.6%  lr: 0.000050  loss: 0.001398  eta: <1min   tot: 0h12m47s  (99.5%)\n",
      " ---+++                Epoch    4 Train error : 0.00138185 +++--- ���\n",
      "Saving model to file : starspace_embeddings\n",
      "Saving model in tsv format : starspace_embeddings.tsv\n"
     ]
    }
   ],
   "source": [
    "#!starspace train -trainFile \"data/train_prepared.tsv\" -model starspace_embeddings -adagrad true -ngrams 1 -lr 0.05 \\\n",
    "#-epoch 5 -dim 100 -negSearchLimit 10 -trainMode 3 -similarity \"cosine\" -minCount 2 -fileFormat labelDoc \\\n",
    "#-verbose true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_starspace_embeddings(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = read_starspace_embeddings('starspace_embeddings.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.521 | Hits@   1: 0.521\n",
      "DCG@   5: 0.615 | Hits@   5: 0.696\n",
      "DCG@  10: 0.633 | Hits@  10: 0.752\n",
      "DCG@ 100: 0.665 | Hits@ 100: 0.905\n",
      "DCG@ 500: 0.675 | Hits@ 500: 0.979\n",
      "DCG@1000: 0.677 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 90\t94\t73\t53\t9\t43\t57\t69\t31\t91\t11\t8\t24\t42\t85\t55\t6\t35\t13\t72\t70\t21\t26\t17\t87\t58\t49\t66\t71\t4\t32\t63\t46\t59\t84...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = \"data/test_prepared.tsv\" ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n",
      "Task StarSpaceRanks: 90\t94\t73\t53\t9\t43\t57\t69\t31\t91\t11\t8\t24\t42\t85\t55\t6\t35\t13\t72\t70\t21\t26\t17\t87\t58\t49\t66\t71\t4\t32\t63\t46\t59\t84...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"bhaskarjitsarmah@gmail.com\" # EMAIL \n",
    "STUDENT_TOKEN = \"ZWzvxUvwsUm3lmJB\" # TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
